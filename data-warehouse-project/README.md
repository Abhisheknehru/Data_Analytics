# ğŸ“Š Data Warehouse for Customer and Product Analytics

A comprehensive data warehouse solution implementing a **Star Schema** architecture for customer and product analytics. This project demonstrates end-to-end data warehouse design, ETL implementation, and analytics capabilities.

![Project Status](https://img.shields.io/badge/status-active-success.svg)
![Database](https://img.shields.io/badge/database-PostgreSQL-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## ğŸ¯ Project Overview

This project implements a production-ready data warehouse using dimensional modeling techniques to support:
- Customer behavior analysis
- Product performance tracking
- Sales trend analysis
- Promotion effectiveness measurement
- Regional performance monitoring

## ğŸ—ï¸ Architecture

### Star Schema Design
```
                    Dim_Date
                       |
    Dim_Customer --- Fact_Sales --- Dim_Product
                       |
                   Dim_Store
                       |
                  Dim_Promotion
```

The warehouse consists of:
- **1 Fact Table**: `fact_sales` (transactional sales data)
- **5 Dimension Tables**: Customer, Product, Store, Date, Promotion
- **Slowly Changing Dimensions**: Type 2 implementation for Customer and Product

## ğŸ“ Project Structure

```
data-warehouse-project/
â”œâ”€â”€ README.md                 # Project overview and documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .gitignore               # Git ignore rules
â”œâ”€â”€ LICENSE                  # MIT License
â”‚
â”œâ”€â”€ sql/                     # SQL scripts
â”‚   â”œâ”€â”€ schema/
â”‚   â”‚   â”œâ”€â”€ create_dimensions.sql
â”‚   â”‚   â”œâ”€â”€ create_facts.sql
â”‚   â”‚   â””â”€â”€ create_indexes.sql
â”‚   â”œâ”€â”€ sample_data/
â”‚   â”‚   â””â”€â”€ insert_sample_data.sql
â”‚   â””â”€â”€ queries/
â”‚       â”œâ”€â”€ analytics_queries.sql
â”‚       â””â”€â”€ performance_queries.sql
â”‚
â”œâ”€â”€ scripts/                 # Python ETL scripts
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â””â”€â”€ scd_handler.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ db_connector.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ validators.py
â”‚   â””â”€â”€ generate_sample_data.py
â”‚
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ database_config.yaml
â”‚   â””â”€â”€ etl_config.yaml
â”‚
â”œâ”€â”€ notebooks/               # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_sales_analysis.ipynb
â”‚   â””â”€â”€ 03_customer_segmentation.ipynb
â”‚
â”œâ”€â”€ data/                    # Sample data files
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ etl_process.md
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â””â”€â”€ deployment_guide.md
â”‚
â””â”€â”€ tests/                   # Unit tests
    â”œâ”€â”€ test_etl.py
    â””â”€â”€ test_validators.py
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- PostgreSQL 12+ (or any SQL database)
- pip package manager

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/data-warehouse-project.git
cd data-warehouse-project
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure database connection**
```bash
cp config/database_config.yaml.example config/database_config.yaml
# Edit the file with your database credentials
```

4. **Create database schema**
```bash
psql -U your_username -d your_database -f sql/schema/create_dimensions.sql
psql -U your_username -d your_database -f sql/schema/create_facts.sql
psql -U your_username -d your_database -f sql/schema/create_indexes.sql
```

5. **Generate and load sample data**
```bash
python scripts/generate_sample_data.py
python scripts/etl/load.py
```

## ğŸ“Š Database Schema

### Fact Table: fact_sales
Primary transactional table containing sales metrics:
- sales_key (PK)
- date_key, customer_key, product_key, store_key, promotion_key (FKs)
- Measures: quantity_sold, sales_amount, cost_amount, profit_amount, etc.

### Dimension Tables

| Dimension | Key Attributes | SCD Type |
|-----------|---------------|----------|
| dim_customer | customer_segment, loyalty_tier, demographics | Type 2 |
| dim_product | category hierarchy, brand, pricing | Type 2 |
| dim_store | location, region, store_type | Type 1 |
| dim_date | calendar attributes, fiscal periods | Static |
| dim_promotion | promotion_type, discount_percentage | Type 1 |

## ğŸ’¡ Key Features

### âœ… Implemented Features
- âœ“ Complete star schema with normalized dimensions
- âœ“ Slowly Changing Dimension (Type 2) handling
- âœ“ Comprehensive ETL pipeline
- âœ“ Data quality validation
- âœ“ Sample data generation
- âœ“ Performance optimization (indexes, partitioning)
- âœ“ Analytical query examples
- âœ“ Documentation and data dictionary

### ğŸ“ˆ Analytics Capabilities
- Customer lifetime value analysis
- Product performance tracking
- Sales trend analysis (daily, monthly, yearly)
- Customer segmentation
- Promotion effectiveness
- Regional performance comparison
- Year-over-year growth analysis

## ğŸ” Sample Queries

### Top Selling Products
```sql
SELECT 
    p.product_name,
    p.category_level_1,
    SUM(f.sales_amount) as total_revenue,
    SUM(f.quantity_sold) as units_sold
FROM fact_sales f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024 AND p.is_current = TRUE
GROUP BY p.product_name, p.category_level_1
ORDER BY total_revenue DESC
LIMIT 10;
```

### Customer Segmentation Analysis
```sql
SELECT 
    c.customer_segment,
    COUNT(DISTINCT c.customer_key) as customer_count,
    AVG(f.sales_amount) as avg_transaction_value,
    SUM(f.profit_amount) as total_profit
FROM fact_sales f
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE c.is_current = TRUE
GROUP BY c.customer_segment
ORDER BY total_profit DESC;
```

More queries available in `sql/queries/analytics_queries.sql`

## ğŸ› ï¸ ETL Pipeline

The ETL process follows these steps:

1. **Extract**: Pull data from source systems (CSV, API, databases)
2. **Transform**: 
   - Data cleansing and validation
   - Type conversions
   - Business rule application
   - SCD Type 2 processing
3. **Load**: Bulk insert to staging, then to warehouse tables

```bash
# Run the complete ETL pipeline
python scripts/etl/extract.py
python scripts/etl/transform.py
python scripts/etl/load.py
```

## ğŸ“ˆ Performance Optimization

### Indexing Strategy
- Primary keys on all tables
- Foreign keys in fact tables
- Commonly filtered columns (date, customer_segment, category)

### Partitioning
- Fact tables partitioned by date (monthly)
- Improves query performance for time-based analysis

### Materialized Views
Pre-aggregated summaries for common queries:
- Monthly sales summary
- Customer segment aggregates
- Product category rollups

## ğŸ§ª Testing

Run unit tests:
```bash
python -m pytest tests/
```

## ğŸ“š Documentation

Detailed documentation available in the `docs/` folder:
- [Architecture Overview](docs/architecture.md)
- [ETL Process](docs/etl_process.md)
- [Data Dictionary](docs/data_dictionary.md)
- [Deployment Guide](docs/deployment_guide.md)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your LinkedIn](https://linkedin.com/in/yourprofile)

## ğŸ™ Acknowledgments

- Star Schema design based on Ralph Kimball's dimensional modeling methodology
- Inspired by real-world data warehouse implementations

## ğŸ“ Support

For support, email your.email@example.com or open an issue in the GitHub repository.

---

**â­ If you find this project helpful, please consider giving it a star!**
